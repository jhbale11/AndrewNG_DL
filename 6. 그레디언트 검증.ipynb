{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Checking\n",
    "\n",
    "jhbale11\n",
    "\n",
    "[Coursera]Andrew Ng 's Deep Learning Lecture\n",
    "\n",
    "https://github.com/suqi/deeplearning_andrewng\n",
    "\n",
    "앤드류 응 교수님의 딥러닝 강의를 듣고 예제를 제 방식대로 연습한 결과입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "from utils_6 import sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) How does gradient checking work?\n",
    "\n",
    "역전파는 gradients를 계산할 것이며, gradients의 요소들은 순전파를 통해 구해집니다.\n",
    "\n",
    "순전파는 비교적 구현이 쉽기 때문에, 역전파에 비해 올바르게 구현했을 가능성이 높습니다. 따라서 순전파만을 이용해 역전파를 근사할 수 있다면 역전파를 제대로 구현했는지 좀 더 확신할 수 있을 것입니다. \n",
    "\n",
    "다음 공식을 수치적으로 계산하면 순전파를 이용해 기울기를 근사적으로 구할 수 있습니다.\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 1-dimensional gradient checking\n",
    "\n",
    "우선 쉬운 예제로서 $J(\\theta) = \\theta x$에 대해 gradient checking을 해볼 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, theta):\n",
    "    J = theta * x\n",
    "    return J\n",
    "\n",
    "def backward_propagation(x, theta):\n",
    "    dtheta = x\n",
    "    return dtheta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에 있는 (1) 공식을 이용해 $\\theta$에 대한 미분값을 근사적으로 계산하고, 이를 역전파를 통해 구한 값과 비교해야 합니다. 두 값의 차이는 다음 공식을 이용해 계산합니다.\n",
    "\n",
    "- $\\theta^{+} = \\theta + \\varepsilon$\n",
    "- $\\theta^{-} = \\theta - \\varepsilon$\n",
    "- $J^{+} = J(\\theta^{+})$\n",
    "- $J^{-} = J(\\theta^{-})$\n",
    "$$ difference = \\frac {\\mid\\mid grad - gradapprox \\mid\\mid_2}{\\mid\\mid grad \\mid\\mid_2 + \\mid\\mid gradapprox \\mid\\mid_2} \\tag{2}$$\n",
    "- 만약 차이가 작다면 (가령 $10^{-7}$보다 작다면), 역전파를 올바르게 구현했다고 자신해도 좋습니다! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(x, theta, epsilon = 1e-7):\n",
    "    theta_plus = theta + epsilon\n",
    "    theta_minus = theta - epsilon\n",
    "    J_plus = forward_propagation(x, theta_plus)\n",
    "    J_minus = forward_propagation(x, theta_minus)\n",
    "    \n",
    "    gradapprox = (J_plus - J_minus) / (2*epsilon)\n",
    "    grad = backward_propagation(x, theta)\n",
    "    \n",
    "    difference = np.linalg.norm(grad - gradapprox) / (np.linalg.norm(grad) + np.linalg.norm(gradapprox))\n",
    "    \n",
    "    if difference < 1e-7:\n",
    "        print (\"The gradient is correct!\")\n",
    "    else:\n",
    "        print (\"The gradient is wrong!\")\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient is correct!\n",
      "difference = 2.919335883291695e-10\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "difference = gradient_check(x, theta)\n",
    "print(\"difference = \" + str(difference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) N-dimensional gradient checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "이제 좀 더 일반화된 경우를 연습하기 위해 3층 신경망에 대한 gradient checking을 구현해보겠습니다. \n",
    "\n",
    "이번에 사용할 DNN은 다음과 같이 구성되어 있습니다.\n",
    "1) LINEAR -> RELU\n",
    "2) LINEAR -> RELU\n",
    "3) LINEAR -> SIGMOID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_n(X, Y, parameters):\n",
    "    m = X.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "    \n",
    "    cost = -np.sum(Y * np.log(A3) + (1 - Y) * np.log(1-A3)) / m\n",
    "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return cost, cache\n",
    "\n",
    "def backward_propagation_n(X, Y, cache):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = np.dot(dZ3, A2.T) / m\n",
    "    db3 = np.sum(dZ3, axis=1, keepdims=True) / m\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = dA2 * (Z2>0)\n",
    "    dW2 = np.dot(dZ2, A1.T) / m\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = dA1 * (Z1>0)\n",
    "    dW1 = np.dot(dZ1, X.T) / m\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "        \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전과 마찬가지 방식으로 그레이언트 검사를 수행합니다. \n",
    "\n",
    "단 이번에는 검사해야 할 파라미터의 개수가 많습니다. 각각의 파라미터 모두에 대해 위에서 했던 방식을 반복하기 위해서, 모든 파라미터를 하나의 백터로 만들어서 계산하는 방식을 사용해보겠습니다. 이를 위해 사전에 정의된 `dictionary_to_vector` 등의 함수를 이용합니다 dictionary_to_vector 함수는 Utils 파일에 정의해두었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check_n(parameters, gradients, X, Y, epsilon = 1e-7):\n",
    "    \n",
    "    # 모든 파라미터(\"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\")를 (-1, 1) 크기를 가진 하나의 벡터로 만든다.\n",
    "    parameters_values, _ = dictionary_to_vector(parameters)\n",
    "    \n",
    "    # 모든 그레디언트(backward_propagation_n의 반환값)를 (-1, 1) 크기를 가진 하나의 벡터로 만든다.\n",
    "    grad = gradients_to_vector(gradients)\n",
    "    \n",
    "    num_parameters = parameters_values.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "    \n",
    "    # gradapprox 계산하기\n",
    "    for i in range(num_parameters):\n",
    "        theta_plus = np.copy(parameters_values)\n",
    "        theta_plus[i, 0] += epsilon\n",
    "        J_plus, _ = forward_propagation_n(X, Y, vector_to_dictionary(theta_plus))\n",
    "\n",
    "        theta_minus = np.copy(parameters_values)\n",
    "        theta_minus[i, 0] -= epsilon\n",
    "        J_minus, _ = forward_propagation_n(X, Y, vector_to_dictionary(theta_minus))\n",
    "        gradapprox[i] = (J_plus - J_minus) / (2*epsilon)\n",
    "        \n",
    "    difference = np.linalg.norm(grad - gradapprox) / (np.linalg.norm(grad)+np.linalg.norm(gradapprox))\n",
    "\n",
    "    if difference > 2e-7:\n",
    "        print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    else:\n",
    "        print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mYour backward propagation works perfectly fine! difference = 2.989007775278216e-09\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 잘 작동하는지 확인하기 위해 임의의 값들을 넣어서 확인.\n",
    "def gradient_check_n_test_case(): \n",
    "    x = np.random.randn(4,3)\n",
    "    y = np.array([1, 1, 0])\n",
    "    W1 = np.random.randn(5,4) \n",
    "    b1 = np.random.randn(5,1) \n",
    "    W2 = np.random.randn(3,5) \n",
    "    b2 = np.random.randn(3,1) \n",
    "    W3 = np.random.randn(1,3) \n",
    "    b3 = np.random.randn(1,1) \n",
    "    parameters = {\"W1\": W1,\"b1\": b1,\"W2\": W2,\"b2\": b2,\"W3\": W3,\"b3\": b3}\n",
    "    return x, y, parameters\n",
    "\n",
    "X, Y, parameters = gradient_check_n_test_case()\n",
    "\n",
    "cost, cache = forward_propagation_n(X, Y, parameters)\n",
    "gradients = backward_propagation_n(X, Y, cache)\n",
    "difference = gradient_check_n(parameters, gradients, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그레디언트를 수치적으로 계산하는 것은 느리기 때문에 코드가 제대로 짜여졌는지 확인하는 용도로만 사용하는 것이 좋습니다.\n",
    "- 드롭아웃을 사용할 경우, 우선 드롭아웃이 없는 채로 그레디언트 검증을 사용하고 드롭아웃을 추가하여야합니다."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "deep-neural-network",
   "graded_item_id": "n6NBD",
   "launcher_item_id": "yfOsE"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
